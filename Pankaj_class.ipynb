{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800026c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ItsMe\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d16f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get or create pyspark session\n",
    "def getSpark(appName):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1323bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730091ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539978ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e87284",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ebc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -ls /user/itv002193/pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD from textFile\n",
    "\n",
    "test = spark.sparkContext.textFile(\"/user/itv002193/pyspark/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36723d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = spark.sparkContext.wholeTextFiles(\"/user/itv002193/pyspark/test*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03657e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates empty RDD with no partition    \n",
    "empt_rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd488aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(empt_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10, 11,12,12,1221121,212,12,12,12,1,21,1,21,2,12,1,12],10) #This creates 10 partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77643464",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6328f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
    "#Outputs: \"re-partition count:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb88e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.repartition(4)  # Shuffles data from all nodes and then creates additional partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.coalesce(1) # moves data from minimum nodes and creates/reduces partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97b3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through RDD\n",
    "for ele in test.collect():\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatmap function --- Flattens rdd into multiple rows\n",
    "\n",
    "flatRdd = test.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035457d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatRdd.collect()[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Function -- Applies function passed to Map function to all the elements of the rdd\n",
    "\n",
    "\n",
    "mapRdd = test.map(lambda x: x.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapRdd.collect()[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcab02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey Function -- reduces / Sums values based on common keys/elements\n",
    "\n",
    "mapRdds = test.map(lambda x: (x, 1))\n",
    "reducedRdd = mapRdds.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b72e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda64ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapRdds.collect()[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99894a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortByKey -- sorted the pairedrdd by its key\n",
    "\n",
    "sortedRdd = mapRdds.sortByKey()\n",
    "sortedRdd1 = reducedRdd.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function -- filters rdd based on filter function\n",
    "\n",
    "filteredRdd = test.filter(lambda x: 'is' in x)\n",
    "filteredRdd1 = mapRdds.filter(lambda x: 'is' in x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredRdd1.collect()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b008c",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb49468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "rdd = spark.sparkContext.textFile(\"/user/itv002193/pyspark/test.txt\")\n",
    "\n",
    "for element in rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "#Flatmap    \n",
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)\n",
    "#map\n",
    "rdd3=rdd2.map(lambda x: (x,1))\n",
    "for element in rdd3.collect():\n",
    "    print(element)\n",
    "#reduceByKey\n",
    "rdd4=rdd3.reduceByKey(lambda a,b: a+b)\n",
    "for element in rdd4.collect():\n",
    "    print(element)\n",
    "#map\n",
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "for element in rdd5.collect():\n",
    "    print(element)\n",
    "#filter\n",
    "rdd6 = rdd5.filter(lambda x : 'a' in x[1])\n",
    "for element in rdd6.collect():\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1430c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey -- groups the rdd elements by its keys\n",
    "\n",
    "groupedRdd = mapRdds.groupByKey().mapValues(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e05681",
   "metadata": {},
   "outputs": [],
   "source": [
    "cogroupedRdd = mapRdds.cogroup(groupedRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977264e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cogroupedRdd.mapValues(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bddbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b955c2",
   "metadata": {},
   "source": [
    "# Actions "
   ]
  },
  {
   "cell_type": "raw",
   "id": "90c02576",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.countApprox(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6280a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.countApproxDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapRdds.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99172f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedRdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f84f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "  \n",
    "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb66dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Function\n",
    "\n",
    "\n",
    "#aggregate\n",
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg=listRdd.aggregate(0, seqOp, combOp)\n",
    "print(agg) # output 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce -- reduces elements of each partitions\n",
    "from operator import add, sub\n",
    "listRdd.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b71b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1268ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.takeOrdered(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b82f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.takeOrdered(6, key= lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba337679",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.takeSample(False, 5, seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "listRdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun  8 20:06:31 2022\n",
    "\n",
    "@author: ravikant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"Z\", 1), (\"A\", 20), (\"B\", 30), (\"C\", 40), (\"B\", 30), (\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "\n",
    "listRdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 3, 2])\n",
    "\n",
    "# aggregate\n",
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg = listRdd.aggregate(0, seqOp, combOp)\n",
    "print(agg)  # output 20\n",
    "\n",
    "# aggregate 2\n",
    "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "agg2 = listRdd.aggregate((0, 0), seqOp2, combOp2)\n",
    "print(agg2)  # output (20,7)\n",
    "\n",
    "agg2 = listRdd.treeAggregate(0, seqOp, combOp)\n",
    "print(agg2)  # output 20\n",
    "\n",
    "# fold\n",
    "foldRes = listRdd.fold(0, add)\n",
    "print(foldRes)  # output 20\n",
    "\n",
    "# reduce\n",
    "redRes = listRdd.reduce(add)\n",
    "print(redRes)  # output 20\n",
    "\n",
    "# treeReduce. This is similar to reduce\n",
    "\n",
    "\n",
    "def add(x, y): return x + y\n",
    "\n",
    "\n",
    "redRes = listRdd.treeReduce(add)\n",
    "print(redRes)  # output 20\n",
    "\n",
    "# Collect\n",
    "data = listRdd.collect()\n",
    "print(data)\n",
    "\n",
    "#count, countApprox, countApproxDistinct\n",
    "print(\"Count : \"+str(listRdd.count()))\n",
    "# Output: Count : 20\n",
    "print(\"countApprox : \"+str(listRdd.countApprox(1200)))\n",
    "# Output: countApprox : (final: [7.000, 7.000])\n",
    "print(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n",
    "# Output: countApproxDistinct : 5\n",
    "print(\"countApproxDistinct : \"+str(inputRDD.countApproxDistinct()))\n",
    "# Output: countApproxDistinct : 5\n",
    "\n",
    "#countByValue, countByValueApprox\n",
    "print(\"countByValue :  \"+str(listRdd.countByValue()))\n",
    "\n",
    "\n",
    "# first\n",
    "print(\"first :  \"+str(listRdd.first()))\n",
    "# Output: first :  1\n",
    "print(\"first :  \"+str(inputRDD.first()))\n",
    "# Output: first :  (Z,1)\n",
    "\n",
    "# top\n",
    "print(\"top : \"+str(listRdd.top(2)))\n",
    "# Output: take : 5,4\n",
    "print(\"top : \"+str(inputRDD.top(2)))\n",
    "# Output: take : (Z,1),(C,40)\n",
    "\n",
    "# min\n",
    "print(\"min :  \"+str(listRdd.min()))\n",
    "# Output: min :  1\n",
    "print(\"min :  \"+str(inputRDD.min()))\n",
    "# Output: min :  (A,20)\n",
    "\n",
    "# max\n",
    "print(\"max :  \"+str(listRdd.max()))\n",
    "# Output: max :  5\n",
    "print(\"max :  \"+str(inputRDD.max()))\n",
    "# Output: max :  (Z,1)\n",
    "\n",
    "#take, takeOrdered, takeSample\n",
    "print(\"take : \"+str(listRdd.take(2)))\n",
    "# Output: take : 1,2\n",
    "print(\"takeOrdered : \" + str(listRdd.takeOrdered(2)))\n",
    "# Output: takeOrdered : 1,2\n",
    "print(\"take : \"+str(listRdd.takeSample()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eda99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RDD \n",
    "\n",
    "filteredRdd = test.filter(lambda x: 'is' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredRdd.saveAsTextFile(\"/user/itv002193/pyspark/filteredRdd\") ## saves in number of partitions files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0140ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -ls /user/itv002193/pyspark/filteredRdd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -cat /user/itv002193/pyspark/filteredRdd4/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75151e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredRdd.coalesce(1).saveAsTextFile(\"/user/itv002193/pyspark/filteredRdd5\", ) # saves in 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredRdd.saveAsTextFile(\"/user/itv002193/pyspark/filteredRdd5\", compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf606d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat /user/itv002193/pyspark/filteredRdd5/part-00000.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache and Persist in Pyspark\n",
    "\n",
    "\n",
    "cachedRdd = test.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cachedRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b37a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfPersist = test.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfPersist.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersist1 = test.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfPersist1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f1fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74958d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "241ef30b",
   "metadata": {},
   "source": [
    "# Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f921714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "spark = getSpark(\"Pyspark DataFrame Tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rows to be inserted into DataFrame\n",
    "names = [('James',None,'Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000)\n",
    "]\n",
    "\n",
    "# Columns for DataFrame Schema\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "# Create dataFrame from rows and columns above\n",
    "df = spark.createDataFrame(data=names, schema=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2697265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dataFrame\n",
    "\n",
    "# Type of DataFrame\n",
    "print(\"Type of dataFrame is \\n\", type(df))\n",
    "print('\\n')\n",
    "\n",
    "# Get list of columns \n",
    "print(\"Columns in Dataframe are \\n\", df.columns)\n",
    "print('\\n')\n",
    "\n",
    "# Length of DataFrame columns\n",
    "print(\"Number of columns in Dataframe are \\n \", len(df.columns))\n",
    "print('\\n')\n",
    "\n",
    "# Get column datatype\n",
    "print(\"Column Data types are \\n\", df.dtypes)\n",
    "print('\\n')\n",
    "\n",
    "# Get Schema of the DataFrame\n",
    "print(\"DataFrame schema using printSchema function \\n\", df.printSchema())\n",
    "print('\\n')\n",
    "\n",
    "print(\"DataFrame Schema using schema function \\n\", df.schema)\n",
    "print('\\n')\n",
    "\n",
    "# Number of rows in DataFrame\n",
    "print(\"Number of rows in Dataframe are \\n\", df.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Shape / Size of DataFrame \n",
    "print(\"The shape of DataFrame is \" + str(len(df.columns)) + \"x\" + str(df.count()))\n",
    "print(\"The shape of DataFrame is {}x{}\".format(str(len(df.columns)), str(df.count())))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get Stastitical description of the DataFrame\n",
    "print(\"Stastitical Description of the DataFrame\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get Distinct Values\n",
    "print(\"Distinct Values of the DataFrame are \")\n",
    "print(df.distinct())\n",
    "print(\"\\n\")\n",
    "\n",
    "#Get count of distinct values\n",
    "print(\"Count of Distinct values are \")\n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77d752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49235227",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.take(3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "families = df.alias(\"families\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ec04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb90364",
   "metadata": {},
   "outputs": [],
   "source": [
    "family = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6484c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"Johnson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c206c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(fraction=0.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d5a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "617cb467",
   "metadata": {},
   "source": [
    "## DataFrame from external sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403da12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SparkSession\n",
    "spark = getSpark(\"DataFrame with External Sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bc7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from CSV file\n",
    "\n",
    "matches_df = spark.read.csv(path=\"/user/itv002193/pyspark/raw_data/matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8da13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----+-----+-----+-----------+-------------+------+----------+------+-----------+--------------+---------------+-----+-------+-------+-------+\n",
      "|_c0|   _c1| _c2| _c3|  _c4|  _c5|        _c6|          _c7|   _c8|       _c9|  _c10|       _c11|          _c12|           _c13| _c14|   _c15|   _c16|   _c17|\n",
      "+---+------+----+----+-----+-----+-----------+-------------+------+----------+------+-----------+--------------+---------------+-----+-------+-------+-------+\n",
      "| id|season|city|date|team1|team2|toss_winner|toss_decision|result|dl_applied|winner|win_by_runs|win_by_wickets|player_of_match|venue|umpire1|umpire2|umpire3|\n",
      "+---+------+----+----+-----+-----+-----------+-------------+------+----------+------+-----------+--------------+---------------+-----+-------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e62054",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -put /home/itv002193/data/matches.csv /user/itv002193/pyspark/matches.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf117664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv by specifying header\n",
    "matches_df = spark.read.csv(\"/user/itv002193/pyspark/raw_data/matches.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ffb3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+--------+-------+\n",
      "| id|season|     city|    date|              team1|               team2|         toss_winner|toss_decision|result|dl_applied|              winner|win_by_runs|win_by_wickets|player_of_match|               venue|       umpire1| umpire2|umpire3|\n",
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+--------+-------+\n",
      "|  1|  2017|Hyderabad|4/5/2017|Sunrisers Hyderabad|Royal Challengers...|Royal Challengers...|        field|normal|         0| Sunrisers Hyderabad|         35|             0|   Yuvraj Singh|Rajiv Gandhi Inte...|   AY Dandekar|NJ Llong|   null|\n",
      "|  2|  2017|     Pune|4/6/2017|     Mumbai Indians|Rising Pune Super...|Rising Pune Super...|        field|normal|         0|Rising Pune Super...|          0|             7|      SPD Smith|Maharashtra Crick...|A Nand Kishore|  S Ravi|   null|\n",
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed7ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+--------+-------------------+---------------------------+---------------------------+-------------+------+----------+----------------------+-----------+--------------+---------------+-----------------------------------------+--------------+--------+-------+\n",
      "|id |season|city     |date    |team1              |team2                      |toss_winner                |toss_decision|result|dl_applied|winner                |win_by_runs|win_by_wickets|player_of_match|venue                                    |umpire1       |umpire2 |umpire3|\n",
      "+---+------+---------+--------+-------------------+---------------------------+---------------------------+-------------+------+----------+----------------------+-----------+--------------+---------------+-----------------------------------------+--------------+--------+-------+\n",
      "|1  |2017  |Hyderabad|4/5/2017|Sunrisers Hyderabad|Royal Challengers Bangalore|Royal Challengers Bangalore|field        |normal|0         |Sunrisers Hyderabad   |35         |0             |Yuvraj Singh   |Rajiv Gandhi International Stadium, Uppal|AY Dandekar   |NJ Llong|null   |\n",
      "|2  |2017  |Pune     |4/6/2017|Mumbai Indians     |Rising Pune Supergiant     |Rising Pune Supergiant     |field        |normal|0         |Rising Pune Supergiant|0          |7             |SPD Smith      |Maharashtra Cricket Association Stadium  |A Nand Kishore|S Ravi  |null   |\n",
      "+---+------+---------+--------+-------------------+---------------------------+---------------------------+-------------+------+----------+----------------------+-----------+--------------+---------------+-----------------------------------------+--------------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show complete column values\n",
    "matches_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4659ee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af0eb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from Parquet File\n",
    "matches_par = spark.read.parquet(\"/user/itv002193/pyspark/processed_data/matches_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b91eb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+---------+-------+\n",
      "| id|season|     city|    date|              team1|               team2|         toss_winner|toss_decision|result|dl_applied|              winner|win_by_runs|win_by_wickets|player_of_match|               venue|       umpire1|  umpire2|umpire3|\n",
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+---------+-------+\n",
      "|  1|  2017|Hyderabad|4/5/2017|Sunrisers Hyderabad|Royal Challengers...|Royal Challengers...|        field|normal|         0| Sunrisers Hyderabad|         35|             0|   Yuvraj Singh|Rajiv Gandhi Inte...|   AY Dandekar| NJ Llong|   null|\n",
      "|  2|  2017|     Pune|4/6/2017|     Mumbai Indians|Rising Pune Super...|Rising Pune Super...|        field|normal|         0|Rising Pune Super...|          0|             7|      SPD Smith|Maharashtra Crick...|A Nand Kishore|   S Ravi|   null|\n",
      "|  3|  2017|   Rajkot|4/7/2017|      Gujarat Lions|Kolkata Knight Ri...|Kolkata Knight Ri...|        field|normal|         0|Kolkata Knight Ri...|          0|            10|        CA Lynn|Saurashtra Cricke...|   Nitin Menon|CK Nandan|   null|\n",
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_par.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1abc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from ORC File\n",
    "matches_orc = spark.read.orc(\"/user/itv002193/pyspark/processed_data/matches_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be42eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+--------+-------+\n",
      "| id|season|     city|    date|              team1|               team2|         toss_winner|toss_decision|result|dl_applied|              winner|win_by_runs|win_by_wickets|player_of_match|               venue|       umpire1| umpire2|umpire3|\n",
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+--------+-------+\n",
      "|  1|  2017|Hyderabad|4/5/2017|Sunrisers Hyderabad|Royal Challengers...|Royal Challengers...|        field|normal|         0| Sunrisers Hyderabad|         35|             0|   Yuvraj Singh|Rajiv Gandhi Inte...|   AY Dandekar|NJ Llong|   null|\n",
      "|  2|  2017|     Pune|4/6/2017|     Mumbai Indians|Rising Pune Super...|Rising Pune Super...|        field|normal|         0|Rising Pune Super...|          0|             7|      SPD Smith|Maharashtra Crick...|A Nand Kishore|  S Ravi|   null|\n",
      "+---+------+---------+--------+-------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_orc.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ed6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from Json File with 1 record\n",
    "zipcode_json = spark.read.json(\"/user/itv002193/pyspark/raw_data/zipcode1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e721263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----+--------------------+---------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|       City|Country|Decommisioned|  Lat|            Location|   LocationText|  LocationType|  Long|RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-----------+-------+-------------+-----+--------------------+---------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PARC PARQUE|     US|        false|17.96|NA-US-PR-PARC PARQUE|Parc Parque, PR|NOT ACCEPTABLE|-66.22|           1|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "+-----------+-------+-------------+-----+--------------------+---------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zipcode_json.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fa5f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from Json File with multiple records\n",
    "zipcode_mult_json = spark.read.json(\"/user/itv002193/pyspark/raw_data/multiline-zipcode.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18afb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_mult_json.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d1eaaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving DataFrame in Parquet file format\n",
    "matches_df.write.parquet(\"/user/itv002193/pyspark/processed_data/matches_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe2a6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving DataFrame in ORC file format\n",
    "matches_df.write.orc(\"/user/itv002193/pyspark/processed_data/matches_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5fc8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving DataFrame in CSV file format\n",
    "matches_df.write.parquet(\"/user/itv002193/pyspark/processed_data/matches_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fea54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec3e9496",
   "metadata": {},
   "source": [
    "### DataFrame Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1a386d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame using CSV file\n",
    "case_df = spark.read.csv(\"/user/itv002193/pyspark/raw_data/Case.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4996c67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+-----+-------------------+---------+---------+----------+\n",
      "| case_id|province|      city|group|     infection_case|confirmed| latitude| longitude|\n",
      "+--------+--------+----------+-----+-------------------+---------+---------+----------+\n",
      "| 1000001|   Seoul|Yongsan-gu| true|      Itaewon Clubs|      139|37.538621|126.992652|\n",
      "| 1000002|   Seoul| Gwanak-gu| true|            Richway|      119| 37.48208|126.901384|\n",
      "| 1000003|   Seoul|   Guro-gu| true|Guro-gu Call Center|       95|37.508163|126.884387|\n",
      "+--------+--------+----------+-----+-------------------+---------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69b322a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' case_id', 'province', 'city', 'group', 'infection_case', 'confirmed', 'latitude', 'longitude'] \n",
      "\n",
      "[(' case_id', 'int'), ('province', 'string'), ('city', 'string'), ('group', 'boolean'), ('infection_case', 'string'), ('confirmed', 'int'), ('latitude', 'string'), ('longitude', 'string')] \n",
      "\n",
      "StructType(List(StructField( case_id,IntegerType,true),StructField(province,StringType,true),StructField(city,StringType,true),StructField(group,BooleanType,true),StructField(infection_case,StringType,true),StructField(confirmed,IntegerType,true),StructField(latitude,StringType,true),StructField(longitude,StringType,true))) \n",
      "\n",
      "root\n",
      " |--  case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get columns, dataypes, schema of the DataFrame\n",
    "\n",
    "print(case_df.columns, \"\\n\")\n",
    "print(case_df.dtypes, \"\\n\")\n",
    "print(case_df.schema, \"\\n\")\n",
    "print(case_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29861773",
   "metadata": {},
   "source": [
    "### Change Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "189bb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumnRenamed(\" case_id\", \"case_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c430718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_no: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e412233",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumnRenamed(\"infection_case\", \"infection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ef1d8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_no: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91ff3d",
   "metadata": {},
   "source": [
    "### Iterate through each column of the dataFrame / Print Each column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf260bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_no\n",
      "province\n",
      "city\n",
      "group\n",
      "infection_case\n",
      "confirmed\n",
      "latitude\n",
      "longitude\n"
     ]
    }
   ],
   "source": [
    "for i in case_df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805148cd",
   "metadata": {},
   "source": [
    "### Select columns from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6cb846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|case_id|confirmed|      city|\n",
      "+-------+---------+----------+\n",
      "|1000001|      139|Yongsan-gu|\n",
      "|1000002|      119| Gwanak-gu|\n",
      "+-------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.select(\"case_id\", \"confirmed\", \"city\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c08aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_case_df = case_df.select([\"case_id\", \"confirmed\", \"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35c16a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|case_id|confirmed|      city|\n",
      "+-------+---------+----------+\n",
      "|1000001|      139|Yongsan-gu|\n",
      "|1000002|      119| Gwanak-gu|\n",
      "+-------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_case_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57319ff",
   "metadata": {},
   "source": [
    "### Sorting the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51dbf655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|case_id|  province|           city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+-------+----------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|7000002|   Jeju-do|              -|false|contact with patient|        0|        -|         -|\n",
      "|3000007|Gangwon-do|              -|false|contact with patient|        0|        -|         -|\n",
      "|1000030|     Seoul|     Gangseo-gu| true|SJ Investment Cal...|        0|37.559649|126.835102|\n",
      "|1100007|     Busan|from other city| true|Cheongdo Daenam H...|        1|        -|         -|\n",
      "|1000025|     Seoul|     Gangnam-gu| true|Gangnam Dongin Ch...|        1|37.522331|127.057388|\n",
      "|1000028|     Seoul|from other city| true|Anyang Gunpo Past...|        1|        -|         -|\n",
      "|1700003|    Sejong|from other city| true|  Shincheonji Church|        1|        -|         -|\n",
      "|1300005|   Gwangju|              -|false|                 etc|        1|        -|         -|\n",
      "|1000019|     Seoul|from other city| true|Daejeon door-to-d...|        1|        -|         -|\n",
      "|1000034|     Seoul|              -| true|         Orange Life|        1|        -|         -|\n",
      "+-------+----------+---------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.sort(\"confirmed\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6152d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|case_id|        province|           city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+-------+----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|1200001|           Daegu|         Nam-gu| true|  Shincheonji Church|     4511| 35.84008|  128.5667|\n",
      "|1200009|           Daegu|              -|false|contact with patient|      917|        -|         -|\n",
      "|1200010|           Daegu|              -|false|                 etc|      747|        -|         -|\n",
      "|6000001|Gyeongsangbuk-do|from other city| true|  Shincheonji Church|      566|        -|         -|\n",
      "|2000020|     Gyeonggi-do|              -|false|     overseas inflow|      305|        -|         -|\n",
      "|1000036|           Seoul|              -|false|     overseas inflow|      298|        -|         -|\n",
      "|1200002|           Daegu|   Dalseong-gun| true|Second Mi-Ju Hosp...|      196|35.857375|128.466651|\n",
      "|6000012|Gyeongsangbuk-do|              -|false|contact with patient|      190|        -|         -|\n",
      "|1000037|           Seoul|              -|false|contact with patient|      162|        -|         -|\n",
      "|1000001|           Seoul|     Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|\n",
      "+-------+----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.sort(\"confirmed\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91b5cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|case_id|        province|           city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+-------+----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|1200001|           Daegu|         Nam-gu| true|  Shincheonji Church|     4511| 35.84008|  128.5667|\n",
      "|1200009|           Daegu|              -|false|contact with patient|      917|        -|         -|\n",
      "|1200010|           Daegu|              -|false|                 etc|      747|        -|         -|\n",
      "|6000001|Gyeongsangbuk-do|from other city| true|  Shincheonji Church|      566|        -|         -|\n",
      "|2000020|     Gyeonggi-do|              -|false|     overseas inflow|      305|        -|         -|\n",
      "|1000036|           Seoul|              -|false|     overseas inflow|      298|        -|         -|\n",
      "|1200002|           Daegu|   Dalseong-gun| true|Second Mi-Ju Hosp...|      196|35.857375|128.466651|\n",
      "|6000012|Gyeongsangbuk-do|              -|false|contact with patient|      190|        -|         -|\n",
      "|1000037|           Seoul|              -|false|contact with patient|      162|        -|         -|\n",
      "|1000001|           Seoul|     Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|\n",
      "+-------+----------------+---------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.sort(case_df.confirmed.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da6ca737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|case_id|  province|           city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+-------+----------+---------------+-----+--------------------+---------+---------+----------+\n",
      "|7000002|   Jeju-do|              -|false|contact with patient|        0|        -|         -|\n",
      "|3000007|Gangwon-do|              -|false|contact with patient|        0|        -|         -|\n",
      "|1000030|     Seoul|     Gangseo-gu| true|SJ Investment Cal...|        0|37.559649|126.835102|\n",
      "|1100007|     Busan|from other city| true|Cheongdo Daenam H...|        1|        -|         -|\n",
      "|1000025|     Seoul|     Gangnam-gu| true|Gangnam Dongin Ch...|        1|37.522331|127.057388|\n",
      "|1000028|     Seoul|from other city| true|Anyang Gunpo Past...|        1|        -|         -|\n",
      "|1700003|    Sejong|from other city| true|  Shincheonji Church|        1|        -|         -|\n",
      "|1300005|   Gwangju|              -|false|                 etc|        1|        -|         -|\n",
      "|1000019|     Seoul|from other city| true|Daejeon door-to-d...|        1|        -|         -|\n",
      "|1000034|     Seoul|              -| true|         Orange Life|        1|        -|         -|\n",
      "+-------+----------+---------------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.sort(case_df.confirmed.asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f488dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Functions from pyspark sql module by aliasing it as 'F'\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a536d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-----+--------------------+---------+--------+---------+\n",
      "|case_id|province|  city|group|      infection_case|confirmed|latitude|longitude|\n",
      "+-------+--------+------+-----+--------------------+---------+--------+---------+\n",
      "|1200001|   Daegu|Nam-gu| true|  Shincheonji Church|     4511|35.84008| 128.5667|\n",
      "|1200009|   Daegu|     -|false|contact with patient|      917|       -|        -|\n",
      "|1200010|   Daegu|     -|false|                 etc|      747|       -|        -|\n",
      "+-------+--------+------+-----+--------------------+---------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.sort(F.desc(\"confirmed\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "52fde1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-----+--------------------+---------+---------+----------+\n",
      "|case_id|  province|      city|group|      infection_case|confirmed| latitude| longitude|\n",
      "+-------+----------+----------+-----+--------------------+---------+---------+----------+\n",
      "|1000030|     Seoul|Gangseo-gu| true|SJ Investment Cal...|        0|37.559649|126.835102|\n",
      "|7000002|   Jeju-do|         -|false|contact with patient|        0|        -|         -|\n",
      "|3000007|Gangwon-do|         -|false|contact with patient|        0|        -|         -|\n",
      "+-------+----------+----------+-----+--------------------+---------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.sort(F.asc(\"confirmed\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce6730",
   "metadata": {},
   "source": [
    "### Creating new column & casting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e6ae90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumn(\"location\", F.concat_ws(',', case_df.latitude, case_df.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b02f219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- location: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5eefc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumn(\"new_case_id\", (case_df.case_id * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6145d09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- location: string (nullable = false)\n",
      " |-- new_case_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14b9c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+-----+--------------------+---------+---------+----------+--------------------+-----------+\n",
      "|case_id|province|        city|group|      infection_case|confirmed| latitude| longitude|            location|new_case_id|\n",
      "+-------+--------+------------+-----+--------------------+---------+---------+----------+--------------------+-----------+\n",
      "|1000001|   Seoul|  Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|37.538621,126.992652|   10000010|\n",
      "|1000002|   Seoul|   Gwanak-gu| true|             Richway|      119| 37.48208|126.901384| 37.48208,126.901384|   10000020|\n",
      "|1000003|   Seoul|     Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|37.508163,126.884387|   10000030|\n",
      "|1000004|   Seoul|Yangcheon-gu| true|Yangcheon Table T...|       43|37.546061|126.874209|37.546061,126.874209|   10000040|\n",
      "+-------+--------+------------+-----+--------------------+---------+---------+----------+--------------------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee7cdb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumn(\"new_case_id\", F.concat_ws(\"-\", case_df.province, case_df.case_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9fd454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- location: string (nullable = false)\n",
      " |-- new_case_id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f28a0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|case_id|province|        city|group|      infection_case|confirmed| latitude| longitude|            location|  new_case_id|\n",
      "+-------+--------+------------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|1000001|   Seoul|  Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|37.538621,126.992652|Seoul-1000001|\n",
      "|1000002|   Seoul|   Gwanak-gu| true|             Richway|      119| 37.48208|126.901384| 37.48208,126.901384|Seoul-1000002|\n",
      "|1000003|   Seoul|     Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|37.508163,126.884387|Seoul-1000003|\n",
      "|1000004|   Seoul|Yangcheon-gu| true|Yangcheon Table T...|       43|37.546061|126.874209|37.546061,126.874209|Seoul-1000004|\n",
      "+-------+--------+------------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "712eec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting the columns\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c87e8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumn(\"confirmed\", case_df.confirmed.cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a72baae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- location: string (nullable = false)\n",
      " |-- new_case_id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "34673939",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = case_df.withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13b3734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- confirmed: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- location: string (nullable = false)\n",
      " |-- new_case_id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9393a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+-----+-------------------+---------+---------+----------+--------------------+-------------+\n",
      "|case_id|province|      city|group|     infection_case|confirmed| latitude| longitude|            location|  new_case_id|\n",
      "+-------+--------+----------+-----+-------------------+---------+---------+----------+--------------------+-------------+\n",
      "|1000001|   Seoul|Yongsan-gu| true|      Itaewon Clubs|      139|37.538621|126.992652|37.538621,126.992652|Seoul-1000001|\n",
      "|1000002|   Seoul| Gwanak-gu| true|            Richway|      119| 37.48208|126.901384| 37.48208,126.901384|Seoul-1000002|\n",
      "|1000003|   Seoul|   Guro-gu| true|Guro-gu Call Center|       95|37.508163|126.884387|37.508163,126.884387|Seoul-1000003|\n",
      "+-------+--------+----------+-----+-------------------+---------+---------+----------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e2e33",
   "metadata": {},
   "source": [
    "### Filter DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "109ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Filter Condition\n",
    "seoul_case_df = case_df.filter(F.col(\"province\") == \"Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21804b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|case_id|province|           city|group|      infection_case|confirmed| latitude| longitude|            location|  new_case_id|\n",
      "+-------+--------+---------------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|1000001|   Seoul|     Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|37.538621,126.992652|Seoul-1000001|\n",
      "|1000002|   Seoul|      Gwanak-gu| true|             Richway|      119| 37.48208|126.901384| 37.48208,126.901384|Seoul-1000002|\n",
      "|1000003|   Seoul|        Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|37.508163,126.884387|Seoul-1000003|\n",
      "|1000004|   Seoul|   Yangcheon-gu| true|Yangcheon Table T...|       43|37.546061|126.874209|37.546061,126.874209|Seoul-1000004|\n",
      "|1000005|   Seoul|      Dobong-gu| true|     Day Care Center|       43|37.679422|127.044374|37.679422,127.044374|Seoul-1000005|\n",
      "|1000006|   Seoul|        Guro-gu| true|Manmin Central Ch...|       41|37.481059|126.894343|37.481059,126.894343|Seoul-1000006|\n",
      "|1000007|   Seoul|from other city| true|SMR Newly Planted...|       36|        -|      null|                 -,-|Seoul-1000007|\n",
      "|1000008|   Seoul|  Dongdaemun-gu| true|       Dongan Church|       17|37.592888|127.056766|37.592888,127.056766|Seoul-1000008|\n",
      "|1000009|   Seoul|from other city| true|Coupang Logistics...|       25|        -|      null|                 -,-|Seoul-1000009|\n",
      "|1000010|   Seoul|      Gwanak-gu| true|     Wangsung Church|       30|37.481735|126.930121|37.481735,126.930121|Seoul-1000010|\n",
      "+-------+--------+---------------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seoul_case_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2d3ab9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Filter Condition\n",
    "seoul_confirmed_80_df = case_df.filter((F.col(\"province\") == \"Seoul\") & (F.col(\"confirmed\") >= 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "92e542f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|case_id|province|      city|group|      infection_case|confirmed| latitude| longitude|            location|  new_case_id|\n",
      "+-------+--------+----------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|1000001|   Seoul|Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|37.538621,126.992652|Seoul-1000001|\n",
      "|1000002|   Seoul| Gwanak-gu| true|             Richway|      119| 37.48208|126.901384| 37.48208,126.901384|Seoul-1000002|\n",
      "|1000003|   Seoul|   Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|37.508163,126.884387|Seoul-1000003|\n",
      "|1000036|   Seoul|         -|false|     overseas inflow|      298|        -|      null|                 -,-|Seoul-1000036|\n",
      "|1000037|   Seoul|         -|false|contact with patient|      162|        -|      null|                 -,-|Seoul-1000037|\n",
      "|1000038|   Seoul|         -|false|                 etc|      100|        -|      null|                 -,-|Seoul-1000038|\n",
      "+-------+--------+----------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seoul_confirmed_80_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e6a7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul_confirmed_80_df = case_df.where((F.col(\"province\") == \"Seoul\") & (F.col(\"confirmed\") >= 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8d6ab66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|case_id|province|      city|group|      infection_case|confirmed| latitude| longitude|            location|  new_case_id|\n",
      "+-------+--------+----------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "|1000001|   Seoul|Yongsan-gu| true|       Itaewon Clubs|      139|37.538621|126.992652|37.538621,126.992652|Seoul-1000001|\n",
      "|1000002|   Seoul| Gwanak-gu| true|             Richway|      119| 37.48208|126.901384| 37.48208,126.901384|Seoul-1000002|\n",
      "|1000003|   Seoul|   Guro-gu| true| Guro-gu Call Center|       95|37.508163|126.884387|37.508163,126.884387|Seoul-1000003|\n",
      "|1000036|   Seoul|         -|false|     overseas inflow|      298|        -|      null|                 -,-|Seoul-1000036|\n",
      "|1000037|   Seoul|         -|false|contact with patient|      162|        -|      null|                 -,-|Seoul-1000037|\n",
      "|1000038|   Seoul|         -|false|                 etc|      100|        -|      null|                 -,-|Seoul-1000038|\n",
      "+-------+--------+----------+-----+--------------------+---------+---------+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seoul_confirmed_80_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c2ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
